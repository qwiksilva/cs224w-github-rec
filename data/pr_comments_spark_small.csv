,pr_id,github_pr_id,commenter_id,head_commit_author_id,head_commit_committer_id,base_commit_author_id,base_commit_committer_id,head_repo_id,base_repo_id,base_repo_owner,head_repo_owner,head_commit_id,base_commit_id,pullreq_id,comment_created_at,comment_position,comment_id,comment_body
0,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-08 21:16:06+00:00,139,41581661,I think that I thought that this block of code was a bit messy while I was working on it. Will clean up while addressing any code review comments.
1,9771161,9038,117201,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-14 22:25:19+00:00,104,42071456,"seems it should be `[key, value]`. Will fix it while merging it."
2,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-09 18:34:26+00:00,131,41675080,"Ah, good point. I'll update to reflect this."
3,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-08 21:20:31+00:00,307,41581951,"I think that the issue here was that some aggregate _did_ call `update()` on the UnsafeRow, leading to problems. I'll go ahead and confirm this, though."
4,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-09 19:24:51+00:00,4,41679635,"Whoops, good catch. I removed this from `planAggregateWithOneDistinct` and it led to a failure in `single distinct column set`:    ```  [info]   == Results ==  [info]   !== Correct Answer - 4 ==                                                        == Sp"
5,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-12 21:49:39+00:00,54,41813748,Perhaps. Let me see if I'm convinced that the other tests are still valid even if we skip this distinct case.
6,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-08 21:09:55+00:00,4,41581219,"To explain what's going on here: it turns out that the refactoring in #8973 to lift this into `InterpretedAggregate` ended up introducing a bug due to lazy val initialization issues. To avoid this, I just pushed these copies to the leaves of our inheritan"
7,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-14 19:56:08+00:00,337,42058972,I'm folding this into the comments right now. Thanks for summarizing.
8,9771161,9038,117201,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-12 21:38:38+00:00,54,41813019,"Should we create a PR to include changes we made before we deleted this check? So, we can merge those changes earlier first and then spend time to figure out the current issue. What do you think?"
9,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-12 16:50:50+00:00,38,41788573,"One thought: instead of trying to preserve  `aggBufferAttributes` and `inputAggBufferAttributes` when returning a new instance, what if we also updated the other expressions which referenced those attributes? Maybe that would be easier."
10,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-08 21:21:01+00:00,351,41582004,Here's the other TungstenAggregate-specific change.
11,9771161,9038,117201,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-11 18:14:59+00:00,38,41714857,"Yeah, it will be good to return a new instance when we call `with*Offset`. For UDAF, I think it may make sense to add a `newInstance` interface so, if a user-defined one has internal states, we can create copies of it  at executor side. There is one diffi"
12,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-11 04:01:47+00:00,38,41705892,Good catch! I wonder whether we can address this treating ImperativeAggregtate instances as immutable and changing the `with*Offset` methods to return new instances. This seems a bit easier / safer than having to remember to clone.
13,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-08 21:15:34+00:00,131,41581609,"Whoops, I meant to clean this up. One concern that I have is whether it's safe to expose an UnsafeRow aggregation buffer to InterpretedAggregate functions. Even if UnsafeRow supports in-place updates for all of the data types used by the aggregate functio"
14,9771161,9038,117201,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-09 16:21:29+00:00,78,41661979,Maybe we should add comments to `allImperativeAggregateFunctionPositions` and `expressionAggInitialProjection` to explain why we do not need to update them after we update `allAggregateFunctions`?
15,9771161,9038,117201,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-09 17:17:14+00:00,328,41667789,"This `update` prevent us from using `UnsafeRow`, right?"
16,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-08 21:10:59+00:00,5,41581301,"These changes in `AggregationIterator.scala` are just some minor optimizations motivated by things that I noticed while working on the Tungsten version. Here, and down below, I noticed that we might be unnecessarily boxing via our use of GenericMutableRow"
17,9771161,9038,117201,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-14 16:27:23+00:00,337,42032759,"Had a discussion with @JoshRosen offline. Here is the explaining for this change.    Our `ImperativeAggregate` and `DeclarativeAggregate` are quite different. For an `ImperativeAggregate`, its correctness is based on the correctness of `mutableAggBufferOf"
18,9771161,9038,117201,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-09 16:20:57+00:00,78,41661916,"Since it will be a `var`, let's double check places where we use it and make sure they will be updated after we change it. I did a quick search, seems `allImperativeAggregateFunctionPositions` and `expressionAggInitialProjection` are the only two `val`s t"
19,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-08 21:17:42+00:00,162,41581748,"While `imperativeAggregateFunctions` can't as easily be defined at the top of the class (since it would need to be reset when switching to spilling), I think that I _could_ lift it out of this `match` statement and put it at the top of this function to cu"
20,9771161,9038,117201,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-09 14:06:51+00:00,131,41647560,"When we do hash-based aggregation, the buffer is an UnsafeRow anyway. So, I guess it is fine to let InterpretedAggregate functions to work with UnsafeRows directly. When a developer adds an new InterpretedAggregate function whose aggregate buffer data typ"
21,9771161,9038,117201,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-09 14:48:06+00:00,4,41651685,"Can we also remove it from `planAggregateWithOneDistinct`?    For `planAggregateWithoutPartial`, we can add the flag `usesTungstenAggregate` and whenever possible use `TungstenAggregate` (we can do it in a separate pr.)."
22,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-09 19:26:30+00:00,4,41679774,This also reminds me: are there planner tests that should be updated to assert that we're actually using the new Tungsten path? Let me check and see.
23,9771161,9038,117201,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-11 02:09:15+00:00,38,41704981,I tried  ```  val originalFunc = allAggregateExpressions(i).aggregateFunction  val baos = new ByteArrayOutputStream()  val funcOut = new ObjectOutputStream(baos)        funcOut.reset()  funcOut.writeObject(originalFunc)  val byteArray = baos.toByteArray  
24,9771161,9038,117201,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-09 17:55:51+00:00,328,41671522,Will https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/udaf.scala#L99-L202 help?
25,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-08 21:12:33+00:00,32,41581411,This is one of the key changes: we need to be able to re-initialize the imperative aggregate functions after we fall back to the sort-based path.
26,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-10 18:18:21+00:00,72,41701104,"I did a bit of refactoring here to try to add some assertions to clarify what's going on (namely, show that there is only one column expression for the distinct aggregates)."
27,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-09 20:03:19+00:00,328,41682767,"As discussed offline, let's not make this change as part of this patch. There's a good chance that calling UnsafeRow's specific setters via this loop may wind up being just as slow as using a SpecificMutableRow. Note that this operator will still _output_"
28,9771161,9038,117201,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-09 16:04:47+00:00,351,41660046,Maybe add a comment at here?
29,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-09 17:51:12+00:00,328,41671105,"Oh, right; this was on my list of things to discuss. I think that we can handle this by calling the specific setter methods because we happen to know the data type. We might want to do this by building up an array of functions to perform the update, simil"
30,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-11 05:27:01+00:00,38,41706713,"I guess that this would also help to avoid potential issues caused by executor-side mutable state in the aggregate functions, should they wind up being used multiple times in different parts of a pipeline."
31,9771161,9038,6682,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-08 21:11:27+00:00,23,41581336,"Here, I noticed that `expressionAggEvalProjection` is always targeted at `aggregateResult`, so I decided to do the targeting once outside of the loop."
32,9771161,9038,117201,6682,6682,6682,117201,1110,8196280,13369,6682,328687714,328665310,9038,2015-10-09 14:16:11+00:00,4,41648516,"So, if anyone tries to move it back to `ImperativeAggregate`, he/she will see an test failure? Should we add a comment to `ImperativeAggregate` to explain why a leaf class needs to implement this?"
33,9796864,9066,117201,6682,6682,6391022,231317,1110,8196280,13369,6682,329396877,329244541,9066,2015-10-11 18:09:23+00:00,54,41714812,Maybe we can avoid of these two `copy`s?
34,9796864,9066,6682,6682,6682,6391022,231317,1110,8196280,13369,6682,329396877,329244541,9066,2015-10-11 18:15:23+00:00,54,41714859,Good catch. We only need these copies in `initialize` since we're buffering the result there. I'll fix this shortly.
35,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-07 21:58:07+00:00,144,55279952,"`level-deserialized` specifies that this block's in-memory values will be stored as Java objects. For a block at this storage level, there are three possibilities:    1. it's stored as java objects in memory  2. it's stored as bytes on disk  3. an error h"
36,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:46:01+00:00,188,55617424,"To be more specific, we don't put the bytes into memory here because a) the block is stored at a deserialized level, so per our contract it should _only_ be stored as values in memory and not bytes, and b) the caller of this method (`doGetLocalBytes()`) h"
37,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:52:30+00:00,537,55618498,"This is in `doPutBytes`. Similar to another comment I had above, there's no reason why we shouldn't attempt to drop it to disk here if it's left. Right now if `memoryStore.putIterator` doesn't fit in memory then we sometimes drop it to disk but sometimes "
38,12809109,11534,988573,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-07 22:00:54+00:00,318,55280347,Why two levels of options? What does it mean if the outer is set but the inner is not
39,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-09 23:41:34+00:00,127,55610821,do we need to release in finally?
40,12809109,11534,988573,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-09 05:13:51+00:00,83,55472870,Comment function and guarantees on _bytes on return.
41,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 01:08:42+00:00,188,55620146,Comment added.
42,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:50:51+00:00,723,55618229,I'll just move this code inside of the `try` block.
43,12809109,11534,988573,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-07 22:39:47+00:00,16,55285972,This not thread safe correct?
44,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:40:13+00:00,395,55616774,I would call this `failedIterator` or something to be more specific.
45,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:43:30+00:00,749,55617134,I would just return `iteratorFromFailedMemoryStorePut` here directly. Doing the `Some(opt.get)` is a strange pattern. If you want you can also add an `assert(blockWasSuccessfullyStored == iteratorFromFailedMemoryStorePut.isEmpty)`.
46,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 01:02:42+00:00,188,55619576,"yeah, that's what I meant"
47,12809109,11534,988573,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-07 21:49:08+00:00,94,55278386,This seems like confusing putIterator semantics...
48,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-08 19:16:56+00:00,345,55411268,I think this was a carryover from the very old code where we synchronized on the BlockInfo objects themselves instead of using our new lock interface. I'll remove this to clean things up.
49,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:58:39+00:00,537,55619226,If `isRight` here returns `false` then in the `if (!putSucceeded && level.useDisk) {` branch four lines down we'll try to put the original bytes to disk.
50,12809109,11534,988573,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-09 05:27:38+00:00,582,55473498,"Can you comment on any properties of the returned iterator in this case?    Just so I understand, if this returns None, we've walked the input iterator to the end. If it returns an iterator, it has been advanced arbitrarily far along?"
51,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-09 23:55:21+00:00,155,55612253,I considered this but thought that doing so would be more confusing to readers.
52,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 19:52:16+00:00,110,55739221,"Yeah, I'm working on this now."
53,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-08 19:22:41+00:00,36,55412172,"Good catch. In the old code, there were multiple places that called into the MemoryStore, some in the BlockManager and others in the CahceManager, so I think we needed these locks to ensure thread-safety. Now, though, I think that all accesses should be c"
54,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:43:46+00:00,149,55617159,That's correct.
55,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 01:11:44+00:00,155,55620372,Done.
56,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 19:31:16+00:00,110,55735901,"@JoshRosen by the way this comment still needs to be addressed, but I think everything else is covered."
57,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 20:10:54+00:00,110,55741963,I agree. I've updated this to put the bytes into memory.
58,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:46:01+00:00,656,55617427,This is the only hesitation about this patch that I have. There's now a non-trivial amount of code that's duplicated in `doPutIterator` and `doPutBytes`. Is there any way to abstract it? Otherwise I'm not sure if we have a net reduction in complexity here
59,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:59:35+00:00,203,55619301,"Sure, I can move it back. This method is going away anyways in my next patch, though."
60,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:40:33+00:00,393,55616812,nit: just put these in 1 line. It fits.
61,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:30:16+00:00,155,55615659,"I think it's pretty clear, but we don't have to do it"
62,12809109,11534,988573,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-07 22:39:27+00:00,172,55285926,is this return code useful?
63,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:31:25+00:00,288,55615777,can you rename this `getRemoteValues` to be consistent? Now we have  ```  getLocalValues  getLocalBytes  getRemote  getRemoteBytes  ```  so this one stands out
64,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-09 23:51:38+00:00,155,55611895,"don't know if we care about the debug message, but a nicer way to write this is:  ```  blockInfoManager.lockForReading(blockId).map { info => doGetLocalBytes(blockId, info) }  ```"
65,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-07 22:03:33+00:00,318,55280741,This was discussed extensively on https://github.com/apache/spark/pull/11502 and will be fixed once I merge that PR.
66,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:53:24+00:00,749,55618661,Done.
67,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:05:38+00:00,127,55613252,can you add a comment in the javadoc on when the lock will be released?
68,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-08 19:20:18+00:00,16,55411807,"Yes, that's right. Thread-safety is ensured by locking at higher levels of the system."
69,12809109,11534,988573,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-09 05:25:13+00:00,632,55473363,Does this mean this block is now split between the memStore and the diskStore?
70,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-09 23:43:06+00:00,127,55610963,Nope; we want the caller to hold a read lock if this method returns successfully.
71,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-08 19:19:44+00:00,172,55411711,"Removals from the stores are idempotent, so the return value lets the caller check whether the removal actually deleted a block (see usage in `BlockManager.removeBlock()`)."
72,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 19:25:53+00:00,500,55735070,I don't think we ever use this anywhere. It's simpler to just return `blockWasSuccessfullyStored` here so this method can just return the result of `doPut` directly instead of `doPut(...) { ... }.isEmpty`
73,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-08 19:30:57+00:00,36,55413618,"Actually, nevermind: we _do_ need this synchronization to guarantee the memory safety / consistency of the bookkeeping structures themselves. This synchronization is distinct from the per-block locking that's done higher up the stack."
74,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:53:29+00:00,14,55618676,nice!
75,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:58:26+00:00,97,55619205,great! I hated this flag.
76,12809109,11534,988573,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-07 22:02:58+00:00,345,55280667,What is this synchronized protecting? You have an exclusive write lock now no?
77,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-09 06:16:14+00:00,582,55476161,"Yep, that's right. The returned iterator, if present, contains the same elements as the original iterator.    This is going to become a little clearer after yet another followup patch to refactor the equivalent memorystore code."
78,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 01:02:42+00:00,393,55619575,Done.
79,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-09 06:16:52+00:00,632,55476210,"Nope, the entire block is now on disk because `iter` contains all of the elements of the original input `iterator()`."
80,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:24:30+00:00,188,55615052,I would add a comment here saying don't bother putting it back in memory here because we'd have to deserialize the data and that might be slow.
81,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 19:27:20+00:00,624,55735303,no need to be a var  ```  val result = try {    val res = putBody(putBlockInfo)    blockWasSuccessfullyStored = res.isEmpty    res  } finally {    ...  }  ...  result  ```
82,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:38:21+00:00,723,55616553,if `keepReadLock` is false we don't actually hold the read lock here! That's not what `doGetLocalBytes` assumes.
83,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:35:41+00:00,149,55616267,"need to clarify in javadocs the locking semantics of `getLocalValues` and `getLocalBytes`. In particular, both of these methods acquire the read lock. In the iterator case we release the lock when the iterator is drained. In the bytes case the caller is r"
84,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-07 21:51:56+00:00,94,55278923,How would you simplify it? We have to return an iterator on failure because of the one-shot iterator semantics.    Do you think that we should just throw away the partially-unrolled iterator returned from `putIterator()` and perform another disk scan inst
85,12809109,11534,988573,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-09 04:45:10+00:00,36,55471546,Hmm okay. DiskStore also needs to be thread safe right? It just happens it doesn't maintain any state right now that's interesting. I think it's worth commenting so people modifying this know what the expected properties are.
86,12809109,11534,988573,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-07 21:50:15+00:00,141,55278560,Comment that this is called with the read lock taken.
87,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 01:00:38+00:00,537,55619390,"ah ok, never mind"
88,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 19:14:58+00:00,141,55733263,Done.
89,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:49:51+00:00,656,55618054,"I've tried factoring it out into a common method but that method has a weird name and signature and the size of the comment needed to describe it meant that we didn't end up saving much space. Basically, I'd like to defer that cleanup to later since it se"
90,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:23:03+00:00,110,55614897,There's not really any reason why we should put it back in memory only for the deserialized case. If I have `MEM_AND_DISK_SER` then this won't even try to the bytes in memory.
91,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 01:02:58+00:00,395,55619604,"I just removed the variable, since the entire line and the `isEmpty` fit together."
92,12809109,11534,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:47:24+00:00,288,55617710,Done.
93,12809109,11534,988573,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-07 21:54:20+00:00,144,55279346,Is this backwards? If the level is deserialized we serialize it? comment.
94,12809109,11534,988573,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-07 22:43:58+00:00,36,55286562,Comment on thread safety. Why do you need locks for entries?
95,12809109,11534,435801,6682,6682,1425879,478320,1110,8196280,13369,6682,401226290,401225887,11534,2016-03-10 00:58:03+00:00,203,55619176,IIUC this is only called in 1 place. Why bother abstracting it? It's just kind of weird to see that it's releasing pending unroll memory outside the context of putting a block. I actually think this fits better where it used to be.
96,12808934,11533,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401314276,401225887,11533,2016-03-07 20:01:34+00:00,5,55262607,I neglected to do this earlier because this entire block of code is being completely rewritten in another one of my PRs.
97,12808934,11533,988573,6682,6682,1425879,478320,1110,8196280,13369,6682,401314276,401225887,11533,2016-03-07 19:59:35+00:00,5,55262323,Is the below comment stale now?
98,12808934,11533,6682,6682,6682,1425879,478320,1110,8196280,13369,6682,401314276,401225887,11533,2016-03-07 20:01:05+00:00,5,55262527,"Yeah, I'll fix it."
99,16617281,14854,637874,6682,6682,1477465,6102074,1110,8196280,13369,6682,501797809,501753080,14854,2016-09-01 06:18:46+00:00,107,77119021,(the comment)
100,16617281,14854,637874,6682,6682,1477465,6102074,1110,8196280,13369,6682,501797809,501753080,14854,2016-08-30 17:35:37+00:00,37,76842149,Btw there's also nextClearBit().
101,16617281,14854,6682,6682,6682,1477465,6102074,1110,8196280,13369,6682,501797809,501753080,14854,2016-09-01 06:15:45+00:00,107,77118807,"In a nutshell, the issue here is that `submitJob` (and `AsyncRDDActions`, more generally) don't have task completion callbacks to remove the progress bar; if we don't have this here then the console progress bar won't be hidden / removed once the job comp"
102,16617281,14854,6682,6682,6682,1477465,6102074,1110,8196280,13369,6682,501797809,501753080,14854,2016-09-01 06:16:33+00:00,4,77118857,"Ah, good idea. Let me update this tomorrow. "
103,16617281,14854,637874,6682,6682,1477465,6102074,1110,8196280,13369,6682,501797809,501753080,14854,2016-08-30 17:35:13+00:00,62,76842063,It might be clearer to just increment frontier until you get to the next missing result.    ```  if (partitionId == frontier) {    while (!gotEnoughRows && partitionStatuses.get(frontier)) {      frontier += 1  ```    also s/partitionStatuses/completedPar
104,16617281,14854,637874,6682,6682,1477465,6102074,1110,8196280,13369,6682,501797809,501753080,14854,2016-09-01 06:05:52+00:00,107,77118164,Comment on why we need the progress bar?
105,16617281,14854,637874,6682,6682,1477465,6102074,1110,8196280,13369,6682,501797809,501753080,14854,2016-09-01 06:18:36+00:00,107,77119011,Would be good to include this in the code.
106,16617281,14854,6682,6682,6682,1477465,6102074,1110,8196280,13369,6682,501797809,501753080,14854,2016-08-30 19:27:38+00:00,62,76862863,"Yep, that's simpler. Updated."
107,16617281,14854,6682,6682,6682,1477465,6102074,1110,8196280,13369,6682,501797809,501753080,14854,2016-08-29 20:14:46+00:00,75,76678387,"Actually, we might be able to cancel the future from here and thereby simplify the waiting code in the driver thread..."
108,16617281,14854,637874,6682,6682,1477465,6102074,1110,8196280,13369,6682,501797809,501753080,14854,2016-08-30 17:33:40+00:00,75,76841791,Makes sense. Then you could just call `jobFuture.result` in that loop.
109,16617281,14854,6682,6682,6682,1477465,6102074,1110,8196280,13369,6682,501797809,501753080,14854,2016-08-28 19:26:56+00:00,4,76537638,This comment was outdated now that we have removed driver-local execution.
110,16617281,14854,637874,6682,6682,1477465,6102074,1110,8196280,13369,6682,501797809,501753080,14854,2016-09-01 06:12:16+00:00,4,77118597,"Maybe adaptiveTake, like the confs right above?"
111,16617281,14854,6682,6682,6682,1477465,6102074,1110,8196280,13369,6682,501797809,501753080,14854,2016-08-28 19:26:37+00:00,4,76537634,"This is the first name that came to mind for this feature-flag, but I'm sure there's a clearer alternative. Please suggest."
112,16649997,14871,400402,6682,6682,15756,15756,1110,8196280,13369,6682,502548900,502436874,14871,2016-08-30 00:31:07+00:00,20,76713033,"I know it's super verbose, but to minimize how confusing this code is, what about calling this launchedTaskAtCurrentMaxLocality, and renaming maxLocality below to currentMaxLocality?"
113,16649997,14871,6682,6682,6682,15756,15756,1110,8196280,13369,6682,502548900,502436874,14871,2016-08-30 00:46:31+00:00,20,76714430,Good idea; updated.
114,16710423,14907,6102074,6682,6682,15756,15756,1110,8196280,13369,6682,503394649,503364189,14907,2016-09-02 16:50:46+00:00,322,77377425,You can use the `U >: Null` bound right?
115,16710423,14907,6682,6682,6682,15756,15756,1110,8196280,13369,6682,503394649,503364189,14907,2016-09-02 02:03:09+00:00,322,77283758,"Actually, I don't think that we can do this because I don't think that there's a way to set an upper type bound to say that `U` must be a nullable object, so you have to do a dangerous `null.asInstanceOf` cast. And since we're not working with primitives "
116,16710423,14907,6682,6682,6682,15756,15756,1110,8196280,13369,6682,503394649,503364189,14907,2016-09-01 01:00:43+00:00,320,77099804,"Here, `close()` tears down the result set, statement, and connection. While in principle closing the connection should be sufficient to close the statement (which should be sufficient to close the result set), not all JDBC drivers adhere to this contract "
117,16710423,14907,6682,6682,6682,15756,15756,1110,8196280,13369,6682,503394649,503364189,14907,2016-09-02 00:54:02+00:00,181,77279354,The close is handled automatically by the `NextIterator` superclass; see https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/NextIterator.scala#L28
118,16710423,14907,6102074,6682,6682,15756,15756,1110,8196280,13369,6682,503394649,503364189,14907,2016-09-02 16:48:07+00:00,152,77377079,That is a valid assumption.
119,16710423,14907,6682,6682,6682,15756,15756,1110,8196280,13369,6682,503394649,503364189,14907,2016-09-01 00:58:00+00:00,267,77099616,"While most of the changes in this block stem from moving the inner loop into a `JdbcUtils` method, there are one or two non-trivial changes that may impact cleanup in error situations. I'll comment on these changes below in order to help walk through them"
120,16710423,14907,6682,6682,6682,15756,15756,1110,8196280,13369,6682,503394649,503364189,14907,2016-09-02 17:37:34+00:00,338,77384218,"Yeah, this is an unfortunate style carryover from the old code :( "
121,16710423,14907,6682,6682,6682,15756,15756,1110,8196280,13369,6682,503394649,503364189,14907,2016-09-01 00:59:34+00:00,266,77099720,"First, note that the old code was mis-idented, so it might have been easy to miss that the entire method body was defining this iterator."
122,16710423,14907,30593500,6682,6682,15756,15756,1110,8196280,13369,6682,503394649,503364189,14907,2016-09-02 00:43:43+00:00,181,77278500,Why don't we close here ?
123,16710423,14907,6682,6682,6682,15756,15756,1110,8196280,13369,6682,503394649,503364189,14907,2016-09-02 17:39:30+00:00,322,77384493,"Aha, forgot about using lower type bounds for this."
124,16710423,14907,6682,6682,6682,15756,15756,1110,8196280,13369,6682,503394649,503364189,14907,2016-09-01 01:04:34+00:00,152,77100063,"Note that we may now close the `ResultSet` three times in `JDBCRDD`: here, at the `CompletionIterator`, and in the `TaskContext` completion callback. This is fine, though, since the old code already assumed that `rs.close()` was idempotent."
125,16710423,14907,6102074,6682,6682,15756,15756,1110,8196280,13369,6682,503394649,503364189,14907,2016-09-02 16:37:12+00:00,338,77375713,Nit: space before the bracket.
126,16710423,14907,6682,6682,6682,15756,15756,1110,8196280,13369,6682,503394649,503364189,14907,2016-09-01 01:03:13+00:00,147,77099971,"Here, I've used `NextIterator` to simplify the implementation of this code. Take a look at the `NextIterator` source and note the similarities between it and the `hasNext` code in the old `JDBCRDD` iterator."
127,16710423,14907,6682,6682,6682,15756,15756,1110,8196280,13369,6682,503394649,503364189,14907,2016-09-01 01:01:58+00:00,338,77099874,"Note that `close()` is idempotent because it may be called twice, at the end of the task and at the end of iteration. In the old code the end-of-iteration case was handled in `hasNext`; here, it's handled in a `CompletionIterator` defined below."
128,16710423,14907,30593500,6682,6682,15756,15756,1110,8196280,13369,6682,503394649,503364189,14907,2016-09-02 00:28:14+00:00,322,77277372,"Can this be  private def nullSafeConvert[T, U](input: T, f: T => U): U"
129,16710423,14907,6682,6682,6682,15756,15756,1110,8196280,13369,6682,503394649,503364189,14907,2016-09-02 00:31:41+00:00,322,77277642,This is carried over from the old code. Good catch.    Changing this signature might actually improve performance in the `ArrayType` case because Scala should be able to statically determine that it can allocate primitive arrays once it knows the return t
130,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-17 18:46:45+00:00,49,92926811,"We could but I'd rather not: my goal here is to not affect the `task.reaper.enabled=false` case at all, which is going to be necessary in order for me to backport this back to 1.6.x / 2.0.x."
131,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-17 04:03:24+00:00,136,92914957,killTimeoutMs <= 0 will mean this loop will never finish : do we want to defend against that config ?
132,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-17 18:48:03+00:00,136,92926830,This is intentional. The goal is that we'll keep polling and logging until the task exits.
133,19500402,16189,275700,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-11 13:54:19+00:00,116,91856289,"> In the case where we do interrupt, however, the introduction of this polling loop means that we'll interrupt the same task multiple times    My 2c: if the application code doesn't respond to the first interrupt immediately, chances are very low that it "
134,19500402,16189,51168,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-09 18:05:36+00:00,6,91764815,That sentence no verb.
135,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-19 18:17:07+00:00,136,93087921,It's mentioned on line 495:    > `// Monitor the killed task until it exits.`
136,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-18 08:39:02+00:00,39,92936385,"sounds good, I wanted to know what kind of 'hit' we take."
137,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-17 18:53:59+00:00,27,92926933,"Yeah, in practice this particular usage isn't flaky."
138,19500402,16189,275700,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-11 12:55:43+00:00,104,91855204,Maybe the name `killPollingInterval` would be slightly better? Also I think the poll interval should not be larger than `spark.task.killTimeout`.
139,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 20:24:10+00:00,28,91379426,"One corner-case that I just thought of: what should happen if the first call to `killTask` sets `interruptThread = false` and the second call sets it to true? If we used a loading cache naively then we would miss the second call. Instead, it might make se"
140,19500402,16189,51168,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 20:34:36+00:00,28,91381321,"And the more extreme case is if `interruptThread` gets repeatedly reset, sometimes `true` and sometimes `false`.  Probably the only tractable logic will be that if `interruptThread` ever gets set to `true`, then the Task can be interrupted regardless of w"
141,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-09 08:00:05+00:00,116,91672368,"In the case where `interruptThread = false` `taskRunner.kill()` will be idempotent and subsequent calls won't have any effect. In the case where we _do_ interrupt, however, the introduction of this polling loop means that we'll interrupt the same task mul"
142,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 20:38:27+00:00,74,91382001,Added a test.
143,19500402,16189,117201,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-16 23:55:00+00:00,144,92905622,"oh, the notifyAll is used for this, right?"
144,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-17 03:57:15+00:00,39,92914906,What is the average and worst case build time hit for this test in your expts ?
145,19500402,16189,117201,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-16 23:56:58+00:00,110,92905791,"It will be good to explain more about how this class works. It is great that we have comments at specific parts, but a section explains the workflow will be very helpful to future readers."
146,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-18 08:36:58+00:00,94,92936352,"I was not looking at it from performance point of view, but from semantics.  notify implies we expect only single thread to be waiting, notifyAll implies we can expect multiple to be waiting.    If we are unsure if multiple threads could be waiting, then "
147,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-13 20:07:00+00:00,28,92254841,"For maximum conservatism, I agree that it probably makes sense to flag this off by default. Even if we don't have the ""kill the JVM"" behavior it's still useful to have the watchdog to pull thread dumps and write logs to indicate what's happening with the "
148,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-17 18:42:22+00:00,94,92926737,"Typically no, but my thought was that the slight performance impact of `notifyAll()` vs `notify()` would be negligible and that it was safer to avoid the risk of a missing wakeup by using `notifyAll()` vs optimizing for a performance problem that we don't"
149,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-17 18:44:02+00:00,94,92926762,"Basically, my concern here was that if I did optimize to just use `notify()` then we'd risk bugs if someone else came along later and added more `wait()`ing threads.    And actually I think we might be able to wind up with two waiting threads in the case "
150,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-17 18:40:02+00:00,28,92926702,"@mridulm, I'm pretty sure that any performance difference here is practically unmeasurable in real-world situations because the `runningTasks.length` is going to be bounded at some small number based on the number of executor cores / task slots and that's"
151,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-16 23:55:29+00:00,144,92905672,Yes.
152,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 09:56:57+00:00,81,91259169,Use Clock instead ?
153,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 06:48:26+00:00,72,91235062,"On the fence about documenting these publicly, but am willing to do so and appreciate naming suggestions."
154,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-09 07:51:02+00:00,103,91671552,"That said, however, it may be useful to have a finally to perform cleanup in a map that I'm about to add / to guarantee that entries cannot possibly be leaked in local mode.  "
155,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-18 08:34:10+00:00,27,92936319,"Ah, missed the obvious - thanks for clarifying."
156,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-17 03:55:46+00:00,153,92914887,Neat !
157,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-17 18:53:16+00:00,135,92926925,I think that my original concern was about holding `taskRunner.synchronized` while taking the thread dump. We need to synchronize from the time that we check `taskRunner.isFinished` to when we call `taskRunner.wait()` in order to avoid a race where the ta
158,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-13 19:53:13+00:00,28,92252007,"@JoshRosen Agree, off by default (with existing impl in place) would be good."
159,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 06:44:40+00:00,27,91234781,This is slightly ugly but it's needed to avoid a race where this regression test can spuriously pass (and thereby fail to test anything) in case we cancel a task right after it has launched on the executor but before the UDF in the task has actually run.
160,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 17:00:32+00:00,103,91337585,Which finally block are you referring to here? The uncaught exception handler in the executor JVM?
161,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-12 05:51:48+00:00,116,91886287,That's a good point. I'll update this tomorrow to only interrupt once.
162,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-17 18:55:15+00:00,39,92926955,"The test itself usually runs in less than 15 seconds. The worst case could be longer, but it's not significantly worse than similar tests in this suite. This wallclock-based timeout isn't ideal, but it's a lot of work to refactor away from it given the di"
163,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-16 23:56:04+00:00,173,92905724,"Yep, the uncaught exception handler will catch this and force the JVM to exit. This is covered by tests added in this patch."
164,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-19 18:16:31+00:00,135,93087804,I realized that one argument for _not_ changing this code is the concern that the thread-dump could hang / lock in some weird way. The current code eliminates this possibility by dropping the synchronized before taking the dump.
165,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 06:43:15+00:00,36,91234685,This naming scheme was intentionally chosen to match the pattern that we use for sorting threads in the executor thread dump page. I'll manually verify that this worked as expected there.
166,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 06:47:39+00:00,28,91235005,"A careful reviewer will notice that it's possible for `killTask` to be called twice for the same task, either via multiple calls to `killTask` here or via a call to `killTask` followed by a later `killAllTasks` call. I think that this should technically b"
167,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-18 08:38:28+00:00,136,92936371,"Interesting, is that documented someplace ? I might have missed it ..."
168,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-17 03:48:23+00:00,94,92914792,Do we have multiple threads waiting on this monitor ? (nofify vs notifyAll)
169,19500402,16189,117201,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-20 02:42:30+00:00,46,93162832,"We set it to 1 to make sure that we will not kill JVM, right (if we kill JVM, we will remove the application because spark.deploy.maxExecutorRetries is 1.)?"
170,19500402,16189,51168,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 19:11:07+00:00,28,91364564,Or access TaskReapers via something like a guava LoadingCache.
171,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 10:00:30+00:00,103,91259805,Why not move the logging into finally block so that even if there are exceptions in the method we have the status ?  (Makes sense to keep the throw here ofcourse).
172,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 17:00:56+00:00,86,91337695,"Yeah, I meant to use `wait()` (was prototyping this quickly late at night, hence this oversight). Will change now."
173,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-09 07:53:57+00:00,28,91671804,"I found a nice way to handle this case by adding a `taskReaperForTask` map which maps from task id to the most recently created reaper. I then synchronize on this map to implement the ""create a new task reaper if one doesn't exist _or_ if the existing one"
174,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-17 03:50:06+00:00,135,92914831,Can we move this variable ?  finished and taskRunner.isFinished seem to be used interchangeably in the code.
175,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-18 08:37:34+00:00,49,92936360,sounds good
176,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-17 04:04:04+00:00,49,92914965,Do we want to introduce dumping stack for this path ? I really like the idea of doing that !
177,19500402,16189,478320,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-17 05:08:17+00:00,27,92915680,@mridulm that doesn't work since driver and executors are not in the same process.
178,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 06:43:55+00:00,105,91234733,"Even if we did throw an exception here, it wouldn't exit the JVM in local mode because we don't set an uncaught exception handler in local mode (see code further up in this file)."
179,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 06:41:03+00:00,91,91234501,This `try` is a precautionary measure to avoid potential issues where the thread mx bean can't give us a thread dump.
180,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-08 11:43:29+00:00,103,91497744,I meant have a try/finally for the entire run method - and log if unable to kill.  Currently it logs only here - but it is possible to exit the method earlier due to exceptions/etc.
181,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-09 08:15:00+00:00,28,91673751,"By the way, I looked at the current code and I'm actually not sure that the back-to-back `killTask` for the same task can actually happen very easily in the current architecture.    AFICT, the `interruptThread` flag comes from the job group properties at "
182,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 09:46:30+00:00,86,91257339,"Why is the sleep inside synchronized ?  Also, would be good to use wait/notify instead of sleep - so that the TaskReaper's are released proactively as soon as task finishes, so that number of threads in the cached pool does not go very high."
183,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-12 05:53:00+00:00,104,91886381,+1 on the naming suggestion; I'll do this tomorrow.
184,19500402,16189,51168,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-12 18:13:37+00:00,28,92005198,"Yes, it is not at all a likely case.  What is the most likely case does concern me, though.    The default value for `spark.job.interruptOnCancel` is `false`; and from the little bit of discussion on SPARK-17064, it doesn't seem that it will be very easy "
185,19500402,16189,117201,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-16 23:46:14+00:00,94,92904925,How about we also comment the purpose of this `notifyAll`?
186,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-17 03:40:38+00:00,45,92914652,"Nit: Would be great if the execute is outside the synchronized block. Should not be an issue here though, but just paranoia ..."
187,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-13 19:03:26+00:00,28,92241444,"Given the current defaults, it's probably best to flag this feature off by default and then I can turn it on in my production environment. For a bit of context, my motivation for this patch is to add a last-ditch safety mechanism to prevent zombie'd execu"
188,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-09 07:50:09+00:00,103,91671465,"I thought about this and it seems like there are only two possibilities here:    1. We're running in local mode, in which case we don't actually want to throw an exception to kill the JVM and even if we did throw then it would keep on running because ther"
189,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 06:49:12+00:00,74,91235127,My goal here was to let users set this to `-1` to disable killing of the executor JVM. I'll add a test to make sure that this flag actually behaves that way.
190,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-08 11:41:44+00:00,81,91497545,"Exactly - for testing.  If not unit tests, atleast functional tests should be done."
191,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-17 03:59:06+00:00,27,92914921,coutdownlatch for this ?
192,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-17 03:44:15+00:00,28,92914715,"Another note: Compared to earlier, where we were simply running down the values (TaskRunner), here we run down the keys (taskId) and do point lookups for each taskId - it does increase the complexity and to be kept in mind (it is iteration/lookup on a Con"
193,19500402,16189,117201,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-16 23:54:45+00:00,173,92905603,I guess I am not clear how we kill the JVM. Are we using this exception to kill the JVM?
194,19500402,16189,275700,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-11 12:45:23+00:00,137,91855053,"I see we also have `(elapsedTimeMs < killTimeoutMs || killTimeoutMs <= 0)` above, what about extracting `killTimeoutMs > 0 && elapsedTimeMs > killTimeoutMs` to a method, e.g `def timeoutExceeded(): Boolean`? "
195,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 06:42:35+00:00,58,91234634,"Task ids should be unique so therefore this thread name should be unique. Hence, I don't think it's super important to reset the thread's name when returning it to this task thread pool because the thread will just be renamed as soon as it's recycled for "
196,19500402,16189,906894,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 09:56:02+00:00,94,91258960,"This is expensive only to get to a single thread dump.  Why not keep track of thread id, dump only for the single thread (instead of all) and validate threadName is same as expected ?"
197,19500402,16189,6682,6682,6682,603514,603514,1110,8196280,13369,6682,572362604,572364738,16189,2016-12-07 17:49:27+00:00,81,91347493,"I don't think that using `Clock` offers a huge improvement here unless we're going to use it to mock out time in unit tests, which I think may be difficult to do here given the structure of this code and difficultly in dependency injection here."
198,22766677,17437,40383,6682,6682,6102074,6102074,1110,8196280,13369,6682,653779517,653779505,17437,2017-03-27 01:18:34+00:00,4,108078826,I think it was. There is `pyspark.mllib.stat` but no `pyspark.ml.stat`. Seems it is mistakenly missing the non-parity between `mllib` and `ml` sub-modules.
199,22766677,17437,6682,6682,6682,6102074,6102074,1110,8196280,13369,6682,653779517,653779505,17437,2017-03-26 21:23:52+00:00,4,108071973,"@holdenk @viirya, quick question here: I see that this `pyspark.ml.stat` line was added in #16465. The problem with this line is that it seems to cause failures during `python setup.py sdist` packaging because that process searches for a non-existent `pys"
200,12565204,11350,6682,6682,6682,28831,28831,1110,8196280,13369,6682,395441575,395441570,11350,2016-02-25 19:08:22+00:00,56,54146389,"It turns out that `nlist` will return `./` and `../`, so we need to filter those out here."
201,12565204,11350,6682,6682,6682,28831,28831,1110,8196280,13369,6682,395441575,395441570,11350,2016-02-25 21:37:26+00:00,86,54168890,I might need to quote the `spark-*` so that the parameter expansion is done by `lftp` rather than the shell.
202,12565204,11350,6682,6682,6682,28831,28831,1110,8196280,13369,6682,395441575,395441570,11350,2016-02-25 20:01:53+00:00,80,54154973,"The FTP protocol itself does not support symlinks and the older versions of `lftp` don't have the `ln` command. I'm going to work around this by uploading the binaries twice, once to the `latest` folder and once to the other one. With the symlink, at leas"
203,5689377,3832,603514,6682,6682,122044,1868770,1110,8196280,13369,6682,200968980,200966996,3832,2014-12-30 21:47:00+00:00,8,22368527,"Please add this configuration, in the streaming section as well of this page as well. "
204,5689377,3832,603514,6682,6682,122044,1868770,1110,8196280,13369,6682,200968980,200966996,3832,2014-12-30 21:49:06+00:00,25,22368596,"Cant these two lines be collapsed into a single function call, say `isValidationEnabled` ? Reduced duplication of hard to track logic."
205,5689377,3832,603514,6682,6682,122044,1868770,1110,8196280,13369,6682,200968980,200966996,3832,2015-01-05 03:18:36+00:00,29,22447244,"No it isnt, sorry. "
206,5689377,3832,603514,6682,6682,122044,1868770,1110,8196280,13369,6682,200968980,200966996,3832,2015-01-05 03:14:31+00:00,8,22447198,nit: extra space before `However`
207,5689377,3832,603514,6682,6682,122044,1868770,1110,8196280,13369,6682,200968980,200966996,3832,2015-01-05 03:15:56+00:00,29,22447211,"Scala style, isnt the indenting wrong here?"
208,5689377,3832,603514,6682,6682,122044,1868770,1110,8196280,13369,6682,200968980,200966996,3832,2014-12-30 21:46:35+00:00,6,22368516,"Better to say ""jobs generated through Spark Streaming's StreamingContext"" rather than ""scheduler"" and stuff. "
209,5689377,3832,603514,6682,6682,122044,1868770,1110,8196280,13369,6682,200968980,200966996,3832,2015-01-05 03:17:45+00:00,31,22447232,"Will be easier to read if the `new File` is assigned to a variable, and maybe `saveAsHadoopFile` is in two lines. "
210,5689377,3832,603514,6682,6682,122044,1868770,1110,8196280,13369,6682,200968980,200966996,3832,2014-12-30 21:57:57+00:00,33,22368902,"This method is not USED by Spark Streaming. Its used by Spark, but it allows higher-level frameworks (like Spark Streaming) to override the behavior. Also this comment should not go into the details of why Spark Streaming updates it. Just refer to JIRA, a"
211,5689377,3832,603514,6682,6682,122044,1868770,1110,8196280,13369,6682,200968980,200966996,3832,2015-01-05 03:15:02+00:00,11,22447201,nit: extra space before `The following`
212,5697300,3849,6682,6682,6682,231317,140235,1110,8196280,13369,6682,201214821,201213715,3849,2014-12-30 21:22:35+00:00,34,22367600,Good catch; this was a carryover from the old code.
213,5697300,3849,202798,6682,6682,231317,140235,1110,8196280,13369,6682,201214821,201213715,3849,2015-01-12 21:48:25+00:00,18,22829776,"Can we introduce another wrapper here instead? I'm imagine we will be adding more fields to serialize to Mesos executors, and it's a lot easier to maintain a struct then position with types and offsets."
214,5697300,3849,6682,6682,6682,231317,140235,1110,8196280,13369,6682,201214821,201213715,3849,2014-12-30 21:22:57+00:00,6,22367616,"Are the descriptions in TaskContext okay?  If so, I'll just copy those for the parameters."
215,5697300,3849,28831,6682,6682,231317,140235,1110,8196280,13369,6682,201214821,201213715,3849,2014-12-30 21:21:20+00:00,14,22367524,can we type data explicitly?
216,5697300,3849,28831,6682,6682,231317,140235,1110,8196280,13369,6682,201214821,201213715,3849,2014-12-30 21:22:13+00:00,34,22367584,"the toLong's are not necessary, are they?"
217,5697300,3849,6682,6682,6682,231317,140235,1110,8196280,13369,6682,201214821,201213715,3849,2014-12-30 21:08:40+00:00,4,22367053,And here's the corresponding receive-side of the Mesos trickiness.
218,5697300,3849,6682,6682,6682,231317,140235,1110,8196280,13369,6682,201214821,201213715,3849,2014-12-30 21:22:13+00:00,5,22367585,"Good call; I didn't do this for the test code, but this line is in DAGScheduler so it should use named arguments."
219,5697300,3849,6682,6682,6682,231317,140235,1110,8196280,13369,6682,201214821,201213715,3849,2014-12-31 04:48:13+00:00,22,22375124,"Just realized that I can change the `maxFailures` property on `local` instead of having to use `local-cluster`.  Let me make that change, since it's a better practice and will speed up the tests."
220,5697300,3849,6682,6682,6682,231317,140235,1110,8196280,13369,6682,201214821,201213715,3849,2014-12-30 21:08:12+00:00,14,22367027,Here's the Mesos trickiness that I alluded to in the PR description.
221,5697300,3849,28831,6682,6682,231317,140235,1110,8196280,13369,6682,201214821,201213715,3849,2014-12-30 21:20:32+00:00,5,22367485,can we use named argument for the two zeros and the true?
222,5697300,3849,6682,6682,6682,231317,140235,1110,8196280,13369,6682,201214821,201213715,3849,2015-01-12 21:54:50+00:00,18,22830201,That's a good idea; putting the serialization / deserialization code in the same wrapper class will make it much easier to verify that it's correct / test it separately.  I'll push a new commit to do this.
223,5697300,3849,28831,6682,6682,231317,140235,1110,8196280,13369,6682,201214821,201213715,3849,2014-12-30 21:20:53+00:00,6,22367512,would be great to add javadoc here
224,5697300,3849,6682,6682,6682,231317,140235,1110,8196280,13369,6682,201214821,201213715,3849,2014-12-31 01:02:00+00:00,34,22372823,"It turns out that `attemptId` was a long but we only expected it to hold values that could fit in an int, hence this weird `.toLong` in a few places, plus  a few `% Int.MaxValue` modulus calls in various places; I've cleaned this up."