{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SRW_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qwiksilva/cs224w-github-rec/blob/master/SRW_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cugWoIdda1x3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "home = \"/gdrive/My Drive/Colab Notebooks/cs224w-data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x5PmLgnNFzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "from collections import defaultdict\n",
        "from scipy import sparse\n",
        "import time\n",
        "import functools\n",
        "import numpy as np\n",
        "from scipy.optimize import fmin_bfgs, fmin_l_bfgs_b\n",
        "import os\n",
        "import array\n",
        "from datetime import datetime as dt\n",
        "from pprint import pprint\n",
        "import pytz\n",
        "import random\n",
        "import joblib\n",
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoGu6jhKl4XW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_srw_data(path=\"/gdrive/My Drive/Colab Notebooks/cs224w-data/final-data-12062019\"):\n",
        "  \"Load the required files\"\n",
        "\n",
        "  # train_srw_output = joblib.load(os.path.join(path, 'train_srw_output_v2.joblib'))\n",
        "  # train_node_mapping = joblib.load(os.path.join(path, 'train_node_mapping_v2.joblib'))\n",
        "  # test_srw_output= joblib.load(os.path.join(path, 'test_srw_output_v2.joblib'))\n",
        "  # test_node_mapping = joblib.load(os.path.join(path, 'test_node_mapping_v2.joblib'))\n",
        "\n",
        "  train_srw_output = joblib.load(os.path.join(path, 'train_srw_output_v4_2018_only.joblib'))\n",
        "  train_node_mapping = joblib.load(os.path.join(path, 'train_node_mapping_v4_2018_only.joblib'))\n",
        "  test_srw_output= joblib.load(os.path.join(path, 'test_srw_output_v4_2018_only.joblib'))\n",
        "  test_node_mapping = joblib.load(os.path.join(path, 'test_node_mapping_v4_2018_only.joblib'))\n",
        "\n",
        "  train_comments = joblib.load(os.path.join(path, 'train_comments_df_v4_2018_only.joblib'))\n",
        "  test_comments = joblib.load(os.path.join(path, 'test_comments_df_v4_2018_only.joblib'))\n",
        "\n",
        "\n",
        "  dtypes = {\n",
        "    'comment': str,\n",
        "    'commenter_id': int,\n",
        "    'commenter_login': str, \n",
        "    'pr_id': int,\n",
        "    'user_id': int,\n",
        "    'username':str,\n",
        "    'title': str,\n",
        "    'description': str,\n",
        "    'pr_created_at': str,\n",
        "    'comment_created_at': str,\n",
        "  }\n",
        "\n",
        "  comments = pd.concat([train_comments,test_comments])\n",
        "  comments = comments.reset_index()\n",
        "\n",
        "  return train_srw_output, train_node_mapping, test_srw_output, test_node_mapping, comments\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBHDdwZXNIMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_srw_data_structs(time_based_cutoff=False, path=\"/gdrive/My Drive/Colab Notebooks/cs224w-data/final-data-12062019\"):\n",
        "  print('preparing data...')\n",
        "  train_srw_output, train_node_mapping, test_srw_output, test_node_mapping, comments = load_srw_data(path)\n",
        "  cutoff_date = dt(2018, 6, 1)\n",
        "  utc=pytz.UTC\n",
        "  cutoff_date = utc.localize(cutoff_date)\n",
        "  unique_train_prs, unique_test_prs, train_comments, test_comments = split_comments(comments, cutoff_date)\n",
        "\n",
        "  print('getting source prs...')\n",
        "  if time_based_cutoff:\n",
        "    source_prs, D_sets = get_source_prs_time_based(train_comments, cutoff_date)\n",
        "\n",
        "  else:\n",
        "    source_prs = get_source_prs(train_comments)\n",
        "    D_sets = {}\n",
        "    for source_id in source_prs:\n",
        "      existing_link_commenters, D_set = get_source_info(source_id, train_comments)\n",
        "      D_sets[source_id] = D_set\n",
        "\n",
        "  D_sets = map_D_sets_to_idx(D_sets, train_node_mapping)\n",
        "\n",
        "  print('creating graph...')\n",
        "  graph = nx.Graph()\n",
        "  graph = create_bipartite_graph(graph, train_srw_output, D_sets)\n",
        "\n",
        "  L_sets = get_L_sets(graph, D_sets, max_size=100)\n",
        "\n",
        "  remove_D_set_features(train_srw_output, D_sets)\n",
        "\n",
        "  test_D_sets = {}\n",
        "  for pr_id in unique_test_prs:\n",
        "    existing_link_commenters, test_D_set = get_source_info(pr_id, test_comments)\n",
        "    if len(existing_link_commenters) > 1 and len(test_D_set) > 1:\n",
        "      test_D_sets[pr_id] = test_D_set\n",
        "\n",
        "  test_D_sets = map_D_sets_to_idx(test_D_sets, test_node_mapping)\n",
        "\n",
        "  test_graph = nx.Graph()\n",
        "  test_graph = create_bipartite_graph(test_graph, test_srw_output, test_D_sets)\n",
        "  remove_D_set_features(test_srw_output, test_D_sets)\n",
        "  # train_graph, idx_to_future_links, idx_to_no_links, nodelist = create_train_bipartite_graph(train_graph, train_comments, source_prs)\n",
        "\n",
        "  return {\n",
        "      'unique_train_prs': unique_train_prs,\n",
        "      'unique_test_prs': unique_test_prs,\n",
        "      'train_comments': train_comments,\n",
        "      'test_comments': test_comments,\n",
        "      'source_prs': source_prs,\n",
        "      'graph': graph,\n",
        "      'D_sets': D_sets,\n",
        "      'L_sets': L_sets,\n",
        "      'train_features':train_srw_output, \n",
        "      'train_node_mapping':train_node_mapping, \n",
        "      'test_features':test_srw_output, \n",
        "      'test_node_mapping':test_node_mapping,\n",
        "      'test_graph': test_graph,\n",
        "      'test_D_sets':test_D_sets\n",
        "  }\n",
        "\n",
        "\n",
        "def map_D_sets_to_idx(D_sets, node_mapping):\n",
        "  new_D_sets = {}\n",
        "  for pr_id in D_sets:\n",
        "    new_D_set = set()\n",
        "    for commenter_id in D_sets[pr_id]:\n",
        "      new_D_set.add( node_mapping[(commenter_id, 'User')] )\n",
        "    new_D_sets[ node_mapping[(pr_id, 'PR')] ] = new_D_set\n",
        "\n",
        "  return new_D_sets\n",
        "\n",
        "\n",
        "def split_comments(comments, cutoff_date):  \n",
        "  unique_train_prs = set(comments[comments.pr_created_at < cutoff_date]['pr_id'].unique())\n",
        "  unique_test_prs = set(comments[comments.pr_created_at >= cutoff_date]['pr_id'].unique())\n",
        "\n",
        "  train_comments = comments[comments['pr_id'].isin(unique_train_prs)]\n",
        "  test_comments = comments[comments['pr_id'].isin(unique_test_prs)]\n",
        "\n",
        "  # train_comments = comments[comments['comment_created_at'] < cutoff_date]\n",
        "  # test_comments = comments[comments['comment_created_at'] >= cutoff_date]\n",
        "\n",
        "  return unique_train_prs, unique_test_prs, train_comments, test_comments\n",
        "\n",
        "\n",
        "def get_source_prs_time_based(train_comments, cutoff_date):\n",
        "  # after_cutoff = train_comments[train_comments['comment_created_at'] >= cutoff_date].groupby('pr_id')['commenter_id'].unique()\n",
        "  # before_cutoff = train_comments[train_comments['comment_created_at'] < cutoff_date].groupby('pr_id')['commenter_id'].unique()\n",
        "\n",
        "  after_cutoff = train_comments[train_comments['comment_created_at'] >= cutoff_date]['pr_id'].unique()\n",
        "  before_cutoff = train_comments[train_comments['comment_created_at'] < cutoff_date]['pr_id'].unique()\n",
        "\n",
        "  valid_commenters = set(train_comments[train_comments['comment_created_at'] < cutoff_date]['commenter_id'].unique())\n",
        "\n",
        "  source_prs = []\n",
        "  D_sets = {}\n",
        "  for pr_id in after_cutoff:\n",
        "    if pr_id not in before_cutoff:\n",
        "      continue\n",
        "\n",
        "    commenters_after = set(train_comments[(train_comments['comment_created_at'] >= cutoff_date) & (train_comments['pr_id'] == pr_id)]['commenter_id'].unique())\n",
        "    commenters_before = set(train_comments[(train_comments['comment_created_at'] < cutoff_date) & (train_comments['pr_id'] == pr_id)]['commenter_id'].unique())\n",
        "    D_set = (commenters_after - commenters_before).intersection(valid_commenters)\n",
        "    owner = train_comments[train_comments['pr_id'] == pr_id]['user_id'].unique()[0]\n",
        "    if owner in D_set:\n",
        "      D_set.remove(owner)\n",
        "    if len(commenters_before) > 1 and len(D_set) > 1:\n",
        "      source_prs.append(pr_id)\n",
        "      D_sets[pr_id] = D_set\n",
        "\n",
        "  return source_prs, D_sets\n",
        "\n",
        "def get_source_prs(train_comments, min_reviewers=4, max_reviewers=16, num_source_prs=200):\n",
        "  random.seed(42)\n",
        "  candidate_train_prs = train_comments.groupby('pr_id')['commenter_id'].nunique()\n",
        "  candidate_train_prs = set(candidate_train_prs[(candidate_train_prs >= min_reviewers) & (candidate_train_prs <= max_reviewers)].index)\n",
        "  source_prs = random.sample(candidate_train_prs, num_source_prs)\n",
        "  return source_prs\n",
        "\n",
        "def get_source_info(source_id, train_comments):\n",
        "  source_comments = train_comments[train_comments['pr_id'] == source_id].sort_values('comment_created_at')\n",
        "  num_commenters = source_comments['commenter_id'].nunique()\n",
        "  num_existing_links = int(num_commenters / 2)\n",
        "\n",
        "  existing_link_commenters = set()\n",
        "  future_link_commenters = set()\n",
        "  owner = train_comments[train_comments['pr_id'] == source_id]['user_id'].unique()[0]\n",
        "  if owner in source_comments['commenter_id'].unique():\n",
        "    existing_link_commenters.add(owner)\n",
        "  for idx, comment in source_comments.iterrows():\n",
        "    if len(existing_link_commenters) < num_existing_links:\n",
        "      existing_link_commenters.add(comment['commenter_id'])\n",
        "\n",
        "    elif len(existing_link_commenters) == num_existing_links and comment['commenter_id'] not in existing_link_commenters:\n",
        "      future_link_commenters.add(comment['commenter_id'])\n",
        "\n",
        "  return existing_link_commenters, future_link_commenters\n",
        "\n",
        "def create_bipartite_graph(graph, features, D_sets):\n",
        "  features = features['num_comment'].tocoo()\n",
        "  for node1_idx, node2_idx in zip(features.row, features.col):\n",
        "    if node2_idx in D_sets and node1_idx in D_sets[node2_idx]:\n",
        "      continue\n",
        "    if node1_idx in D_sets and node2_idx in D_sets[node1_idx]:\n",
        "      continue\n",
        "\n",
        "    graph.add_edge(node1_idx, node2_idx)\n",
        "\n",
        "  assert nx.bipartite.is_bipartite(graph)\n",
        "  return graph\n",
        "\n",
        "\n",
        "def get_L_sets(graph, D_sets, max_size=None):\n",
        "  L_sets = {}\n",
        "  random.seed(42)\n",
        "  for source_node in D_sets:\n",
        "    path_lengths = nx.single_source_shortest_path_length(graph, source_node, cutoff=3)\n",
        "    L_set = set([node for node in path_lengths if path_lengths[node]==3]) - D_sets[source_node]\n",
        "    if max_size and len(L_set) > max_size:\n",
        "      L_set = random.sample(L_set, max_size)\n",
        "    L_sets[source_node] = L_set\n",
        "  return L_sets\n",
        "\n",
        "def remove_D_set_features(features, D_sets):\n",
        "  for node1 in D_sets:\n",
        "    for node2 in D_sets[node1]:\n",
        "      for feature_matrix in features.values():\n",
        "        feature_matrix[node1, node2] = 0\n",
        "        feature_matrix[node2, node1] = 0\n",
        "\n",
        "def create_train_bipartite_graph(graph, train_comments, source_prs):\n",
        "  non_source_prs = set(train_comments['pr_id'].unique()) - set(source_prs)\n",
        "  non_source_comments = train_comments[train_comments['pr_id'].isin(non_source_prs)]\n",
        "  edges = set()\n",
        "\n",
        "  pr_groups = non_source_comments.groupby('pr_id')['commenter_id'].unique()\n",
        "  for pr_id, commenter_ids in pr_groups.iteritems():\n",
        "    pr_node_id = f\"p{int(pr_id)}\"\n",
        "\n",
        "    for commenter_id in commenter_ids:\n",
        "      commenter_node_id = f\"u{int(commenter_id)}\"\n",
        "      edge = (pr_node_id, commenter_node_id)\n",
        "      edges.add(edge)\n",
        "\n",
        "  nodelist = []\n",
        "  for pr_id in train_comments.sort_values('pr_created_at')['pr_id'].unique().tolist():\n",
        "    pr_node_id = f\"p{int(pr_id)}\"\n",
        "    nodelist.append(pr_node_id)\n",
        "\n",
        "  for commenter_id in train_comments['commenter_id'].unique():\n",
        "    commenter_node_id = f\"u{int(commenter_id)}\"\n",
        "    nodelist.append(commenter_node_id)\n",
        "\n",
        "  idx_to_future_links = {}\n",
        "  for source_id in source_prs:\n",
        "    pr_node_id = f\"p{int(source_id)}\"\n",
        "    existing_link_commenters, future_link_commenters = get_source_info(source_id, train_comments)\n",
        "    idx_to_future_links[()] = future_link_commenters\n",
        "    for commenter_id in existing_link_commenters:\n",
        "      commenter_node_id = f\"u{int(commenter_id)}\"\n",
        "      edge = (pr_node_id, commenter_node_id)\n",
        "      edges.add(edge)\n",
        "\n",
        "  graph.add_edges_from(edges)\n",
        "  assert nx.bipartite.is_bipartite(graph)\n",
        "\n",
        "  idx_to_no_links = {}\n",
        "  for source_id in source_prs:\n",
        "    pr_node_id = f\"p{int(source_id)}\"\n",
        "    path_lengths = nx.single_source_shortest_path_length(graph, pr_node_id, cutoff=3)\n",
        "    no_link_set = set([node for node in path_lengths if path_lengths[node]==3])\n",
        "    idx_to_no_links[nodelist.index(pr_node_id)] = no_link_set - idx_to_future_links[nodelist.index(pr_node_id)]\n",
        "\n",
        "  return graph, idx_to_future_links, idx_to_no_links, nodelist\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjIPNSRdYnfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = create_srw_data_structs(time_based_cutoff=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vZ-9ZYApF8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHdX5uIpposQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_nodes = data['test_comments'].pr_id.unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuWtvzKgxnk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph = data['graph']\n",
        "adjacency = nx.adjacency_matrix(graph, nodelist=range(len(graph)))\n",
        "\n",
        "D_set = data['D_sets']\n",
        "L_set = data['L_sets']\n",
        "features = data['train_features']\n",
        "test_features  = data['test_features']\n",
        "train_nodes = [node for node in D_set]\n",
        "test_D_sets = data['test_D_sets']\n",
        "\n",
        "for k, m in features.items():\n",
        "    features[k] = sparse.csr_matrix(m)\n",
        "\n",
        "for k, m in test_features.items():\n",
        "    test_features[k] = sparse.csr_matrix(m)\n",
        "\n",
        "weights = np.random.uniform(1, 0, [len(features)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QknepF7_OCVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D_set_size = np.array([len(v) for k, v in D_set.items()])\n",
        "print(len(D_set_size), D_set_size.mean(), D_set_size.max())\n",
        "\n",
        "test_D_set_size = np.array([len(v) for k, v in test_D_sets.items()])\n",
        "print(len(test_D_set_size), test_D_set_size.mean(), test_D_set_size.max())\n",
        "\n",
        "L_set_size = np.array([len(v) for k, v in L_set.items()])\n",
        "print(L_set_size.mean(), L_set_size.max())\n",
        "\n",
        "combined_size = np.array([L_set_size[i]*D_set_size[i] for i in range(len(D_set_size))])\n",
        "print(combined_size.mean(), combined_size.max())\n",
        "\n",
        "degrees = []\n",
        "for k in D_set:\n",
        "  degrees.append(graph.degree[k])\n",
        "degrees = np.array(degrees)\n",
        "print(degrees.mean(), degrees.max())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkBpQU5VNN9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# graph = nx.DiGraph()\n",
        "# graph = nx.read_weighted_edgelist(\"kubernetes_comment_network.edgelist\", create_using=graph)\n",
        "# adjacency = nx.adjacency_matrix(graph)\n",
        "# # train_comments = pd.read_csv('kubernetes_train_comments.csv')\n",
        "# # file_paths = pd.read_csv('kubernetes_file_paths_final.csv')\n",
        "# # test_comments = pd.read_csv('kubernetes_test_comments.csv')\n",
        "\n",
        "# #train_comments['comment_time_tics'] = train_comments['comment_created_at'].map(convert_to_tic)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYoO4fGBNPOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# rows, cols = adjacency.nonzero()\n",
        "# count = 0\n",
        "# train_nodes = set()\n",
        "# train_edges = []\n",
        "\n",
        "# for row, col in zip(rows, cols):\n",
        "    \n",
        "#     if len(train_nodes) <= 2 and np.sum(adjacency[row, ]) >= 10:\n",
        "#         train_nodes.add(row)\n",
        "    \n",
        "#     if row in train_nodes:\n",
        "#         train_edges.append((row, col))\n",
        "\n",
        "# D_set = {k:[] for k in train_nodes}\n",
        "# L_set = {k:[] for k in train_nodes}\n",
        "# # \n",
        "# for row, col in train_edges:\n",
        "#     if np.random.uniform() > 0.5:\n",
        "#         adjacency[row, col] = 0\n",
        "#         D_set[row].append(col)\n",
        "\n",
        "# for i in range(adjacency.shape[0]):\n",
        "#     for k in L_set:\n",
        "#         if i not in D_set[k]:\n",
        "#             L_set[k].append(i)\n",
        "            \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcslXgLzOS94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ##Making Features\n",
        "\n",
        "# features = {\n",
        "#     \"ft1\" : sparse.lil_matrix(adjacency.shape),\n",
        "#     \"ft2\" : sparse.lil_matrix(adjacency.shape),\n",
        "# }\n",
        "\n",
        "# weights = np.random.uniform(1, 0, [len(features)])\n",
        "\n",
        "# rows, cols = adjacency.nonzero()\n",
        "# for row, col in zip(rows, cols):\n",
        "#     features[\"ft1\"][row, col] = np.random.uniform(0, 1)\n",
        "#     features[\"ft2\"][row, col] = np.random.uniform(0, 1)\n",
        "    \n",
        "# for k, m in features.items():\n",
        "#     features[k] = sparse.csr_matrix(m)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1JqD60dOWtj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def features2EdgeStrength(fts, wts):\n",
        "    a = sparse.csr_matrix((fts[list(fts)[0]].shape), dtype=float)\n",
        "    for i, k in enumerate(fts):\n",
        "        a = a + fts[k] * wts[i]\n",
        "\n",
        "    logistic_data = (1 + np.exp(-a.data))\n",
        "    logistic = a.copy()\n",
        "    logistic.data = logistic_data\n",
        "    \n",
        "    l2 = np.reshape(logistic, a.shape)\n",
        "    \n",
        "    a.data =  1/logistic.data\n",
        "        \n",
        "    \n",
        "    denominator = a ** 2\n",
        "    weight_derivatives = []\n",
        "    \n",
        "    for k in range(len(wts)):\n",
        "        # print ( logistic.shape, denominator.shape)\n",
        "        weight_derivatives.append(wts[k] * logistic * denominator)\n",
        "        \n",
        "    \n",
        "    #d_inv = sparse.diags([[1.0 / a.getrow(i).sum() for i in range(a.shape[0])]], [0])\n",
        "    return a, weight_derivatives\n",
        "\n",
        "def EdgeStrengthToTransitionMatrices(A, alpha, train_nodes):\n",
        "    A_sum_inv = []\n",
        "    for i in range(A.shape[0]):\n",
        "      s = A.getrow(i).sum()\n",
        "      if s != 0:\n",
        "        A_sum_inv.append(1.0/s)\n",
        "      else:\n",
        "        A_sum_inv.append(0)\n",
        "\n",
        "    A_sum_inv = sparse.diags([A_sum_inv], [0])\n",
        "    T = A_sum_inv.dot(A)\n",
        "\n",
        "    T_matrices = []\n",
        "    restart = sparse.csr_matrix((A.shape), dtype=float)\n",
        "    for n in train_nodes:\n",
        "        # restart = np.zeros(A.shape)\n",
        "        # for i in range(restart.shape[0]):\n",
        "        #     restart[i, n] = 1\n",
        "        # restart = sparse.csr_matrix((A.shape), dtype=float)\n",
        "        restart[n] = 1\n",
        "        T_matrices.append((1-alpha)*T + alpha*restart.transpose())\n",
        "        restart[n] = 0\n",
        "        restart.eliminate_zeros()\n",
        "        \n",
        "    return T_matrices\n",
        "\n",
        "def iterPageRank(pr, trans, epsilon = 1e-4):\n",
        "    pr = sparse.csr_matrix(pr)\n",
        "    count = 0\n",
        "    while True:\n",
        "        pr_new = pr.dot(trans)\n",
        "        # delta = np.sum(abs((pr_new - pr).data))\n",
        "        delta = sparse.linalg.norm(pr_new - pr, ord=1)\n",
        "        pr = pr_new\n",
        "        count += 1\n",
        "        if delta < epsilon:\n",
        "            break\n",
        "    \n",
        "    return pr_new[0]\n",
        "\n",
        "    \n",
        "def costFunc(pl, pd, offset):\n",
        "    return 1.0 / (1 + np.exp(-1.0 * (pl - pd)/offset))\n",
        "    \n",
        "def get_loss(D_set, L_set, weights, A, T_matrices, train_nodes, lmbda, offset = 0.1):\n",
        "  cost = 0\n",
        "  nnodes = A.shape[0]\n",
        "  for i, ind in enumerate(train_nodes):\n",
        "      \n",
        "      pp = np.repeat(1.0/nnodes, nnodes)\n",
        "      pgrank = iterPageRank(pp, T_matrices[i])\n",
        "      \n",
        "      \n",
        "      for d in D_set[ind]:\n",
        "          # deltas = (pgrank[0, L_set[ind]] - pgrank[0, d]\n",
        "          # for d in deltas:\n",
        "              # cost += costFunc(d, offset)\n",
        "          for l in L_set[ind]:\n",
        "              cost += costFunc(pgrank[0, l], pgrank[0, d], offset)  \n",
        "              # cost += 1.0 / (1 + np.exp(-1.0 * (pgrank[0, l] - pgrank[0, d])/offset))                        \n",
        "\n",
        "  loss = lmbda * np.sqrt(sum(wk ** 2 for wk in weights)) + cost \n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF2l6hcCQ_r3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def diffTransition(A, dw, alpha = 0.2):\n",
        "\n",
        "  data_dict = {}\n",
        "  for k in range(len(dw)):\n",
        "    data_dict[k] = {\n",
        "        \"rows\": array.array('i'),\n",
        "        \"cols\": array.array('i'),\n",
        "        \"data\": array.array('f'),\n",
        "    }\n",
        "  nnodes = int(A.shape[0])\n",
        "  row_weight_sums = np.array(A.sum(axis=1)).flatten()\n",
        "  row_wdiff_sums = [np.array(w_diff.sum(axis=1)).flatten() for w_diff in dw]\n",
        "\n",
        "\n",
        "\n",
        "  for i in tqdm(range(nnodes)):\n",
        "\n",
        "      if row_weight_sums[i] == 0:\n",
        "        continue\n",
        "\n",
        "      for k, w_diff in enumerate(dw):\n",
        "\n",
        "            denominator = (row_weight_sums[i] ** -2)\n",
        "            #if denominator \n",
        "            numerator = (dw[k][i, ]*row_weight_sums[i]) - (row_wdiff_sums[k][i]*A[i, ])\n",
        "\n",
        "            current_row =  (numerator * (1 - alpha)) * denominator\n",
        "            current_data = current_row.data\n",
        "\n",
        "            for j, col in enumerate(current_row.indices):\n",
        "              data_dict[k]['rows'].append(i)\n",
        "              data_dict[k]['cols'].append(col)\n",
        "              data_dict[k]['data'].append(current_data[j])\n",
        "            #print(\"rec  \", time.time() - t)\n",
        "\n",
        "  dQ = []\n",
        "  for k in data_dict:\n",
        "    data = data_dict[k]['data']\n",
        "    rows = data_dict[k]['rows']\n",
        "    cols = data_dict[k]['cols']\n",
        "    dQ.append(sparse.csr_matrix((data, (rows, cols)),\n",
        "                             shape=(nnodes, nnodes)))\n",
        "  \n",
        "  return dQ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYcl9vb5HXqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6296X1YLvYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterPageDiff(p, trans, transdiff, epsilon = 1e-3, max_iter = 300):\n",
        "    pdiff = sparse.csr_matrix(p.shape)\n",
        "    p_transdiff = p.dot(transdiff)\n",
        "    it = 0\n",
        "    while True :\n",
        "      pr_diff_new = pdiff.dot(trans) + p_transdiff\n",
        "      delta = np.sum(np.abs(pr_diff_new - pdiff).data)\n",
        "      #delta = sparse.linalg.norm(pr_diff_new - pdiff, ord=1)\n",
        "      pdiff = pr_diff_new\n",
        "      it += 1\n",
        "      if it % 500 == 0:\n",
        "        print(delta)\n",
        "      if delta < epsilon:\n",
        "          \n",
        "          return pr_diff_new[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PmhP_fz2n9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pp = np.repeat(1.0/A.shape[0], A.shape[0]) # may want to initialize pagerank with p from last iteration?\n",
        "pgrank = iterPageRank(pp, T_matrices[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOcwWa7e2vE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterPageDiff(pgrank[0], T_matrices[0], diffQ[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8fuC1RiE5vj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_pass(weights, D_Set, L_set, features, train_nodes, lmbda, alpha, offset):\n",
        "  print(\"calculating EdgeStrength\")\n",
        "  A, dw = features2EdgeStrength(features, weights)\n",
        "  print(\"calculating TransitionMatrices\")\n",
        "  T_matrices = EdgeStrengthToTransitionMatrices(A, alpha, train_nodes)\n",
        "  print(\"calculating loss\")\n",
        "  loss = get_loss(D_set, L_set, weights, A, T_matrices, train_nodes, lmbda, offset)\n",
        "  print(\"finished forward, loss:\", loss)\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3rBPiJqObMU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def costDiff(pl, pd, offset):\n",
        "    return (1.0 / offset) * np.exp(-1.0 * (pl - pd)/offset) * (costFunc(pl, pd, offset) ** 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wZGt1VTEVb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lossDiff(T_matrices, diffQ, trian_nodes, offset, D_set, L_set):\n",
        "  diffWeights = np.array([0] * len(diffQ))\n",
        "\n",
        "  nnodes = T_matrices[0].shape[0]\n",
        "  for i, ind in tqdm(enumerate(train_nodes)):\n",
        "\n",
        "          pp = np.repeat(1.0/nnodes, nnodes) # may want to initialize pagerank with p from last iteration?\n",
        "          pgrank = iterPageRank(pp, T_matrices[i])\n",
        "          pgrank_vec = pgrank.toarray()[0]\n",
        "\n",
        "          lossdiffs = defaultdict(dict)\n",
        "          for d in D_set[ind]:\n",
        "            for l in L_set[ind]:\n",
        "                  lossdiffs[d][l] = costDiff(pgrank_vec[l], pgrank_vec[d], offset)\n",
        "          \n",
        "\n",
        "          for k in range(len(weights)):\n",
        "            nodeDiff = 0\n",
        "            pdiff = np.zeros((nnodes))\n",
        "            pdiff = iterPageDiff(pgrank[0], T_matrices[i], diffQ[k])\n",
        "\n",
        "\n",
        "            pdiff_vec = pdiff.toarray()[0]\n",
        "\n",
        "            #need to pass in D_set and L_set to function?\n",
        "            #tt = time.time()\n",
        "            for d in D_set[ind]:\n",
        "                  for l in L_set[ind]:\n",
        "\n",
        "                      nodeDiff += lossdiffs[d][l]*(pdiff_vec[l] - pdiff_vec[d])\n",
        "\n",
        "\n",
        "            diffWeights[k] += nodeDiff    \n",
        "\n",
        "  return diffWeights   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqr1JIooIwsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_pass(weights, D_Set, L_set, features, train_nodes, lmbda, alpha, offset):\n",
        "\n",
        "  \n",
        "  print(\"calculating EdgeStrength\")\n",
        "  A, dw = features2EdgeStrength(features, weights)\n",
        "  print(\"calculating TransitionMatrices\")\n",
        "  T_matrices = EdgeStrengthToTransitionMatrices(A, alpha, train_nodes)\n",
        "\n",
        "  nnodes = A.shape[0]\n",
        "  print(\"calculating diffTransition\")\n",
        "  diffQ = diffTransition(A, dw)\n",
        "  print(\"calculating lossDiff\")\n",
        "  diffWeights = lossDiff(T_matrices, diffQ, train_nodes, offset, D_set, L_set)\n",
        "\n",
        "      \n",
        "  for k in range(len(diffWeights)):\n",
        "    diffWeights[k] += 2.0 * lmbda * weights[k]\n",
        "\n",
        "  #print (diffWeights)\n",
        "  print(\"finished backward\")\n",
        "  return np.array(diffWeights, dtype='float64')\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q03sDIkpF3T9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backward_pass(D_set, L_set, features, train_nodes, 0.2, 0.3, 0.1, wht)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgi-ISM3GW6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print( diffWeights, weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8G7R_bTLCAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wht"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DNkAdQKFgKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_iter = 500\n",
        "lr = 1e-5\n",
        "wht = weights\n",
        "for i in range(max_iter):\n",
        "  loss =  forward_pass(D_set, L_set, features, train_nodes, lmb, alpha, offset, wht)\n",
        "  diffWeights = backward_pass (D_set, L_set, features, train_nodes, lmb, alpha, offset, wht)\n",
        "  wht = [wht[i] - diffWeights[i] * lr for i in range(len(wht))]\n",
        "  print(\"new weights\", wht)\n",
        "  print((\"iteration %d, loss %f\") % (i, loss))\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp0U5_x1GK1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fprintalpha = 0.3\n",
        "A, dw = features2EdgeStrength(features, weights)\n",
        "T_matrices = EdgeStrengthToTransitionMatrices(A, alpha, train_nodes)\n",
        "diffQ = diffTransition(A, dw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrOZtAHwK_Pe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "offset = 0.1\n",
        "diffWeights = lossDiff(T_matrices, diffQ, train_nodes, offset, D_set, L_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdBXwlVOFtWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "forward_pass(D_set, L_set, features, train_nodes, lmb, alpha, offset, weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO77RyuM5ZWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF1c1PzgPLm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backward_pass(D_set, L_set, features, train_nodes, 0.2, 0.3, 0.1, weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz9dpFjmgQRH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lmb = 1\n",
        "alpha = 0.2\n",
        "offset = 0.1\n",
        "w = np.array(weights,dtype='complex', order='F')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_Fr8_DECeAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "beta_Opt = fmin_l_bfgs_b(func = forward_pass,\n",
        "                         x0 = w, \n",
        "                         fprime=backward_pass,\n",
        "                         args = (D_set, L_set, features, train_nodes, lmb, alpha, offset))\n",
        "                        # approx_grad=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhuwIoNVkcBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind2map = {'User': {},\n",
        "           'PR': {},\n",
        "           }\n",
        "for key in test_node_mapping:\n",
        "  ID, typ =  key\n",
        "  if typ == 'User':\n",
        "    ind2map['User'][test_node_mapping[key]] = ID\n",
        "  else:\n",
        "    ind2map['PR'][test_node_mapping[key]] = ID\n",
        "\n",
        "  #ind2map[test_node_mapping[key][0]] = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxy6koucfcO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_comments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnkKXQ5eeTti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_comments = data['test_comments']\n",
        "seen_prs = {}\n",
        "for pr, group in test_comments.groupby('pr_id'):\n",
        "  if pr not in new_labels:\n",
        "    continue\n",
        "  cur_commenters = set(group.commenter_id.unique())\n",
        "  cur_dset = set(new_labels[pr])\n",
        "\n",
        "  seen_prs[pr] = cur_commenters - cur_dset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIe4NtwJqSPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_nodes = data['unique_test_prs']\n",
        "test_nodes = [test_node_mapping[(k, 'PR')] for k in test_nodes]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YybPg2TdUZvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['test_D_sets']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud1wIhYD17Z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind2map['User'][ind]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqdjZczxr5Fz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = data['test_D_sets']\n",
        "labels[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsQEmKorWBVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKy1ijYoM-BN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_features = data['test_features']\n",
        "test_nodes = labels.keys()#[test_node_mapping[(k, 'PR')] for k in data['unique_test_prs']]\n",
        "preds = getPredictions(test_features, beta_Opt[0], ind2map, test_nodes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY6zK--kVnoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in preds:\n",
        "  preds[i] = sorted(preds[i], reverse = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgU6Kh3pVrpL",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GcjBGeWFMEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74WuFVwfmETr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iter = 0\n",
        "for pr_id, pred in preds.items():\n",
        "    print([cand for score, cand in sorted(pred, key=lambda x: x[0], reverse=True)][:10])\n",
        "    if iter == 10:\n",
        "        break\n",
        "    iter += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7rP8D2Jahyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds[65549][:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnzOL78JauVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_labels = {}\n",
        "for k in labels:\n",
        "  new_id = ind2map['PR'][k]\n",
        "  new_labels[new_id] = [ind2map['User'][ind] for ind in labels[k]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDhWCguzanM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_labels[65558]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN2Pppu0X2Gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_nodes = [test_node_mapping[(k, 'PR')] for k in data['unique_test_prs']]\n",
        "print(sorted(test_nodes))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyPOJNaNZoa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sorted(labels.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOx0096NW0Ns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKvAhPZWoqGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['test_comments'].pr_id.unique()[:2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NiMZPTU45V-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joblib.dump(preds, \"SRW_predictions2.joblib\")\n",
        "joblib.dump(new_labels, \"SRW_labels.joblib\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUTSkATQf8yJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "joblib.dump(preds, \"seen_prs.joblib\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRQD-sRvgPod",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqbGe_KDgdc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uploaded = drive.CreateFile({'title': 'seen_prs.joblib'})\n",
        "uploaded.SetContentFile('seen_prs.joblib')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2tTqJeqOF41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sorted(preds[67084], reverse = True)[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM4Zm5-e494x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getPredictions(features, weights, node_mappings, nodes):\n",
        "  A, _ = features2EdgeStrength(features, weights)\n",
        "  nnodes = A.shape[0]\n",
        "  T_matrices = EdgeStrengthToTransitionMatrices(A, alpha, nodes)\n",
        "  #print(T_matrices[0], T_matrices[1])\n",
        "  pred_dict = {}\n",
        "\n",
        "  for i, row in tqdm(enumerate(nodes)):\n",
        "    curid = node_mappings['PR'][row]\n",
        "    pred_dict[curid] = []\n",
        "    pp = np.repeat(1.0/nnodes, nnodes)\n",
        "    pgrank = iterPageRank(pp, T_matrices[i])\n",
        "    #print (pgrank.shape)\n",
        "    scores = pgrank.toarray()[0]\n",
        "    #ranking = np.argsort(scores)\n",
        "    #rankings, score =  zip(*sorted(zip(scores, index)))\n",
        "    for ind, s in enumerate(scores):\n",
        "        if ind in node_mappings['User'] and ind != row:\n",
        "          pred_dict[curid].append((s, node_mappings['User'][ind])) #= [k for k in zip(scores, list(range(len(scores))))]\n",
        "                                    \n",
        "  return pred_dict\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX-iZ9r8-XcZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(test_node_mapping)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3XvZDIybNiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}